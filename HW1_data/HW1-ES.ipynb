{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an index of the downloaded corpus. The documents are found within the ap89_collection folder in the data .zip file. You will need to write a program to parse the documents and send them to your elasticsearch instance.\n",
    "\n",
    "The corpus files are in a standard format used by TREC. Each file contains multiple documents. The format is similar to XML, but standard XML and HTML parsers will not work correctly. Instead, read the file one line at a time with the following rules:\n",
    "\n",
    "Each document begins with a line containing <DOC> and ends with a line containing </DOC>.\n",
    "The first several lines of a document’s record contain various metadata. You should read the <DOCNO> field and use it as the ID of the document.\n",
    "The document contents are between lines containing <TEXT> and </TEXT>.\n",
    "All other file contents can be ignored.\n",
    "Make sure to index term positions: you will need them later. You are also free to add any other fields to your index for later use. This might be the easiest way to get particular values used in the scoring functions. However, if a value is provided by elasticsearch then we encourage you to retrieve it using the elasticsearch API rather than calculating and storing it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import nltk\n",
    "import math \n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch \n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import OrderedDict\n",
    "\n",
    "global average_doc_length\n",
    "global total_doc\n",
    "global vocab_size\n",
    "global dict_ids\n",
    "global df_term_freq\n",
    "global df_ttf_freq\n",
    "global df_doc_freq\n",
    "global len_of_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Elasticsearch instance\n",
    "es = Elasticsearch()\n",
    "\n",
    "## Initialize stemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "## Get all the docs from the file location\n",
    "file_all = os.getcwd() + \"\\HW1\\AP_DATA\\\\ap89_collection\"\n",
    "\n",
    "## Temove the read me part\n",
    "list_of_all = os.listdir(file_all)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read the data from all sources\n",
    "\n",
    "total_doc = 0\n",
    "dict_temp = []\n",
    "doc_id_list_mapping = {}\n",
    "for doc in list_of_all:\n",
    "    with open(r\".\\HW1\\AP_DATA\\ap89_collection\\\\\" + doc, \"r\") as file_data:\n",
    "        file_data_read = file_data.read().split(\"<DOC>\")\n",
    "    for each_file in file_data_read:\n",
    "        if \"</DOCNO>\" in each_file:\n",
    "            doc_id_content = each_file.split(\"</DOCNO>\")\n",
    "            total_doc += 1\n",
    "            doc_id = doc_id_content[0].strip(\"\\n<DOCNO>\").strip()\n",
    "            doc_id_list_mapping[total_doc] = doc_id\n",
    "            doc_content = doc_id_content[1].split(\"<TEXT>\")[1].strip(\"\\n</TEXT>\\n</DOC>\\n\").strip()\n",
    "            dict_to_iterate = {\"docno\": doc_id, \"text\": doc_content}\n",
    "            dict_temp.append(dict_to_iterate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Save the data in elastic search\n",
    "\n",
    "i = 1\n",
    "for es_doc in dict_temp:\n",
    "    es.index(index = \"ap_dataset\", doc_type = \"document\", id = i, body = es_doc)\n",
    "    i += 1\n",
    "    if i % 1000 == 0:\n",
    "        print (\"AT i: \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alleg measur corrupt public offici government jurisdict worldwid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read queries\n",
    "\n",
    "with open(r\"C:\\Users\\mm199\\IR-hw\\HW1_data\\AP_DATA\\query_desc.51-100.short.txt\", \"r\") as query:\n",
    "    query_list = query.readlines()\n",
    "    \n",
    "## define the common words from queries\n",
    "common_words = ['Document', 'discuss', 'exist','determine', 'current','pay','even','taken','type', 'report', 'describe', 'cite', 'include', 'identify', 'make', 'one', 'must', 'second', 'use', 'side', 'take', 'predict']\n",
    "\n",
    "## get all the words in a query\n",
    "all_query_in_a_list = {i[:2] : nltk.word_tokenize(i[3:]) for i in query_list}\n",
    "\n",
    "## get stop words from nltk\n",
    "stop_words = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "## words of dictionary which are not in stop words or common words\n",
    "words_list = {}\n",
    "for key in all_query_in_a_list:\n",
    "    if all_query_in_a_list[key] != []:\n",
    "        words_list[key] = [ps.stem(i) for i in all_query_in_a_list[key] if i not in stop_words and i not in common_words and re.match(\"^\\w+\",i)]\n",
    "for key in words_list:\n",
    "    words_list[key] = [i for i in words_list[key] if i not in stop_words and i not in common_words and re.match(\"^\\w+\",i)]\n",
    "\n",
    "all_unique_words = list(set([word for query_words in words_list.values() for word in query_words]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to run the queries in the file query_desc.51-100.short.txt, included in the data .zip file. You should run all queries (omitting the leading number) using each of the retrieval models listed below, and output the top 1000 results for each query to an output file. If a particular query has fewer than 1000 documents with a nonzero matching score, then just list whichever documents have nonzero scores.\n",
    "\n",
    "You should write precisely one output file per retrieval model. Each line of an output file should specify one retrieved document, in the following format:\n",
    "\n",
    "<query-number> Q0 <docno> <rank> <score> Exp\n",
    "Where:\n",
    "\n",
    "is the number preceding the query in the query list\n",
    "is the document number, from the <DOCNO> field (which we asked you to index)\n",
    "is the document rank: an integer from 1-1000\n",
    "is the retrieval model’s matching score for the document\n",
    "Q0 and Exp are entered literally\n",
    "Your program will run queries against elasticsearch. Instead of using their built in query engine, we will be retrieving information such as TF and DF scores from elasticsearch and implementing our own document ranking. It will be helpful if you write a method which takes a term as a parameter and retrieves the postings for that term from elasticsearch. You can then easily reuse this method to implement the retrieval models.\n",
    "\n",
    "Implement the following retrieval models, using TF and DF scores from your elasticsearch index, as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get total vocab size\n",
    "\n",
    "body = {\n",
    "  \"size\": 0, \n",
    "  \"aggs\": {\n",
    "    \"vocabSize\": {\n",
    "      \"cardinality\": {\n",
    "        \"field\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "vocab_res = es.search(index = \"ap_dataset\", doc_type = \"document\", body = body)\n",
    "vocab_size = vocab_res[\"aggregations\"]['vocabSize']['value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get average doc length for any doc id \n",
    "\n",
    "doc_id = 1\n",
    "dict_all = es.termvectors(index = \"ap_dataset\", doc_type = \"document\", id = doc_id, term_statistics = True, fields = [\"text\"])\n",
    "average_doc_length = dict_all[\"term_vectors\"][\"text\"][\"field_statistics\"][\"sum_ttf\"]/dict_all[\"term_vectors\"][\"text\"][\"field_statistics\"][\"doc_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get all the data and save it in globally defined data frames\n",
    "\n",
    "def get_terms(word, doc_id):\n",
    "    len_of_doc_id = 0\n",
    "    dict_all = es.termvectors(index = \"ap_dataset\", doc_type = \"document\", id = doc_id, term_statistics = True, fields = [\"text\"])\n",
    "    if dict_all[\"term_vectors\"] != {}:\n",
    "        temp_dict = dict_all[\"term_vectors\"][\"text\"][\"terms\"]\n",
    "    else:\n",
    "        return\n",
    "    for every_word in temp_dict:\n",
    "        len_of_doc_id += temp_dict[every_word][\"term_freq\"] \n",
    "    len_of_doc[\"length\"][doc_id] = len_of_doc_id   \n",
    "    try:     \n",
    "        tfwd = temp_dict[word][\"term_freq\"]\n",
    "        ttf = temp_dict[word][\"ttf\"]\n",
    "    except KeyError:\n",
    "        tfwd = 0\n",
    "        ttf = 0\n",
    "    df_term_freq[word][doc_id] = tfwd\n",
    "    df_ttf_freq[word][doc_id] = ttf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define and initialize pandas dataframe to save term freq, doc freq and ttf\n",
    "\n",
    "list_of_values = list(words_list.values())\n",
    "list_single_item = list(set([i for item in list_of_values for i in item]))\n",
    "\n",
    "df_term_freq = pd.DataFrame(data = 0, index = range(1,total_doc+1), columns = list_single_item)\n",
    "df_ttf_freq = pd.DataFrame(data = 0, index = range(1,total_doc+1), columns = list_single_item)\n",
    "df_doc_freq = pd.DataFrame(data = 0, index = [\"doc_freq\"], columns = list_single_item)\n",
    "len_of_doc = pd.DataFrame(data = 0, index = range(1,total_doc+1), columns = [\"length\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Search es for retrieving doc for each term in a query\n",
    "\n",
    "dict_ids = {}\n",
    "for key in words_list:\n",
    "    list_id = []\n",
    "    for word in words_list[key]:\n",
    "        res = es.search(index = \"ap_dataset\", doc_type = \"document\",size = 1000, body = {\"query\" : {\"match\" : {\"text\" : word}}})\n",
    "        df_doc_freq[word][\"doc_freq\"] = res[\"hits\"][\"total\"]\n",
    "        id_list = [int(each_hit[\"_id\"]) for each_hit in res['hits'][\"hits\"]]\n",
    "        list_id.extend(id_list)\n",
    "    dict_ids[key] = list(set(list_id))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word completed: 85  time taken :  663.5449593067169  for ids  7151\n",
      "Word completed: 59  time taken :  446.7133719921112  for ids  5805\n",
      "Word completed: 56  time taken :  270.5866074562073  for ids  4640\n",
      "Word completed: 71  time taken :  1339.8888409137726  for ids  9692\n",
      "Word completed: 64  time taken :  282.0133726596832  for ids  4892\n",
      "Word completed: 62  time taken :  374.0085747241974  for ids  4604\n",
      "Word completed: 93  time taken :  334.16528725624084  for ids  4900\n",
      "Word completed: 99  time taken :  100.2273941040039  for ids  2854\n",
      "Word completed: 58  time taken :  157.2018280029297  for ids  3226\n",
      "Word completed: 77  time taken :  91.32353138923645  for ids  1829\n",
      "Word completed: 54  time taken :  811.0812311172485  for ids  7980\n",
      "Word completed: 87  time taken :  530.8166315555573  for ids  6677\n",
      "Word completed: 94  time taken :  148.1805226802826  for ids  3155\n",
      "Word completed: 10  time taken :  1312.5897421836853  for ids  9272\n",
      "Word completed: 89  time taken :  412.30075788497925  for ids  5203\n",
      "Word completed: 61  time taken :  181.18350338935852  for ids  3770\n",
      "Word completed: 95  time taken :  180.39260125160217  for ids  3871\n",
      "Word completed: 68  time taken :  1388.7973511219025  for ids  9402\n",
      "Word completed: 57  time taken :  130.8091242313385  for ids  2138\n",
      "Word completed: 97  time taken :  191.47264528274536  for ids  3247\n",
      "Word completed: 98  time taken :  285.1484019756317  for ids  4223\n",
      "Word completed: 60  time taken :  1785.3227076530457  for ids  10960\n",
      "Word completed: 80  time taken :  200.858473777771  for ids  3407\n",
      "Word completed: 63  time taken :  100.11918354034424  for ids  2799\n",
      "Word completed: 91  time taken :  544.9223523139954  for ids  6686\n"
     ]
    }
   ],
   "source": [
    "## Initialize pandas dataframe with actual term frequency values\n",
    "\n",
    "for key in words_list:\n",
    "    t1 = time.time()\n",
    "    for word in set(words_list[key]):\n",
    "        for doc_id in dict_ids[key]:\n",
    "            get_terms(word,doc_id)\n",
    "    print (\"Word completed:\", key,\" time taken : \", (time.time() - t1), \" for ids \", len(dict_ids[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for each model per query\n",
    "\n",
    "def call_each_model(model_name):\n",
    "    store_result_for_each_query = {}\n",
    "    for key in words_list:\n",
    "        result_dict = {}\n",
    "        for doc_id in dict_ids[key]:\n",
    "            result_dict[doc_id] = model_name(words_list[key], doc_id)\n",
    "        store_result_for_each_query[key] = sorted(result_dict.items(), key = lambda x:x[1], reverse = True)[:1000]\n",
    "        print (\"Query completed: \", key)\n",
    "    return store_result_for_each_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define all model \n",
    "\n",
    "def okapi_model (word_term_freq_list, doc_id):\n",
    "    okapi = 0.0\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        okapi += round((tfwd / (tfwd + 0.5 + 1.5 * (len_of_doc[\"length\"][doc_id] / average_doc_length))),4)\n",
    "    return okapi\n",
    "    \n",
    "def tfidf_model(word_term_freq_list, doc_id):  \n",
    "    tfidf = 0.0\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        doc_freq = df_doc_freq[key][\"doc_freq\"]\n",
    "        if doc_freq == 0:\n",
    "            tfidf_val = 0\n",
    "        else:\n",
    "            tfidf_val = math.log(total_doc / doc_freq)\n",
    "        tfidf += round(((tfwd / (tfwd + 0.5 + 1.5 * (len_of_doc[\"length\"][doc_id] / average_doc_length))) * tfidf_val),4)\n",
    "    return tfidf\n",
    "\n",
    "def okapi_bm25(word_term_freq_list, doc_id):  \n",
    "    bm25 = 0.0\n",
    "    tfwq = {}\n",
    "    k1 = 1.2\n",
    "    k2 = 500\n",
    "    b = 0.75\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        tfwq[key] = word_term_freq_list.count(key)\n",
    "        bm25 += round(((math.log((total_doc + 0.5) / (df_doc_freq[key][\"doc_freq\"] + 0.5)))*((tfwd + k1 * tfwd) / (tfwd + k1 * ((1 - b) + b * (len_of_doc[\"length\"][doc_id] / average_doc_length))))*((tfwq[key] + k2 * tfwq[key]) / (tfwq[key] + k2))), 5)\n",
    "    return bm25 \n",
    "\n",
    "def laplace_model(word_term_freq_list, doc_id):  \n",
    "    laplace = 0.0\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        prob = ((tfwd + 1) / (len_of_doc[\"length\"][doc_id] + vocab_size))\n",
    "        laplace += math.log(prob)\n",
    "    return laplace\n",
    "\n",
    "def jm_model(word_term_freq_list, doc_id):\n",
    "    jm = 0.0\n",
    "    epsilon = 0.00000001\n",
    "    lamda = 0.3\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        ttf = df_ttf_freq[key][doc_id]\n",
    "        if tfwd == 0:\n",
    "            tfwd = epsilon\n",
    "            ttf = epsilon\n",
    "        prob_lm = lamda * (tfwd / len_of_doc[\"length\"][doc_id]) + (1 - lamda) * (ttf / vocab_size)\n",
    "        jm += math.log(prob_lm)\n",
    "    return jm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query completed:  85\n",
      "Query completed:  59\n",
      "Query completed:  56\n",
      "Query completed:  71\n",
      "Query completed:  64\n",
      "Query completed:  62\n",
      "Query completed:  93\n",
      "Query completed:  99\n",
      "Query completed:  58\n",
      "Query completed:  77\n",
      "Query completed:  54\n",
      "Query completed:  87\n",
      "Query completed:  94\n",
      "Query completed:  10\n",
      "Query completed:  89\n",
      "Query completed:  61\n",
      "Query completed:  95\n",
      "Query completed:  68\n",
      "Query completed:  57\n",
      "Query completed:  97\n",
      "Query completed:  98\n",
      "Query completed:  60\n",
      "Query completed:  80\n",
      "Query completed:  63\n",
      "Query completed:  91\n",
      "Query completed:  85\n",
      "Query completed:  59\n",
      "Query completed:  56\n",
      "Query completed:  71\n",
      "Query completed:  64\n",
      "Query completed:  62\n",
      "Query completed:  93\n",
      "Query completed:  99\n",
      "Query completed:  58\n",
      "Query completed:  77\n",
      "Query completed:  54\n",
      "Query completed:  87\n",
      "Query completed:  94\n",
      "Query completed:  10\n",
      "Query completed:  89\n",
      "Query completed:  61\n",
      "Query completed:  95\n",
      "Query completed:  68\n",
      "Query completed:  57\n",
      "Query completed:  97\n",
      "Query completed:  98\n",
      "Query completed:  60\n",
      "Query completed:  80\n",
      "Query completed:  63\n",
      "Query completed:  91\n",
      "Query completed:  85\n",
      "Query completed:  59\n",
      "Query completed:  56\n",
      "Query completed:  71\n",
      "Query completed:  64\n",
      "Query completed:  62\n",
      "Query completed:  93\n",
      "Query completed:  99\n",
      "Query completed:  58\n",
      "Query completed:  77\n",
      "Query completed:  54\n",
      "Query completed:  87\n",
      "Query completed:  94\n",
      "Query completed:  10\n",
      "Query completed:  89\n",
      "Query completed:  61\n",
      "Query completed:  95\n",
      "Query completed:  68\n",
      "Query completed:  57\n",
      "Query completed:  97\n",
      "Query completed:  98\n",
      "Query completed:  60\n",
      "Query completed:  80\n",
      "Query completed:  63\n",
      "Query completed:  91\n",
      "Query completed:  85\n",
      "Query completed:  59\n",
      "Query completed:  56\n",
      "Query completed:  71\n",
      "Query completed:  64\n",
      "Query completed:  62\n",
      "Query completed:  93\n",
      "Query completed:  99\n",
      "Query completed:  58\n",
      "Query completed:  77\n",
      "Query completed:  54\n",
      "Query completed:  87\n",
      "Query completed:  94\n",
      "Query completed:  10\n",
      "Query completed:  89\n",
      "Query completed:  61\n",
      "Query completed:  95\n",
      "Query completed:  68\n",
      "Query completed:  57\n",
      "Query completed:  97\n",
      "Query completed:  98\n",
      "Query completed:  60\n",
      "Query completed:  80\n",
      "Query completed:  63\n",
      "Query completed:  91\n",
      "Query completed:  85\n",
      "Query completed:  59\n",
      "Query completed:  56\n",
      "Query completed:  71\n",
      "Query completed:  64\n",
      "Query completed:  62\n",
      "Query completed:  93\n",
      "Query completed:  99\n",
      "Query completed:  58\n",
      "Query completed:  77\n",
      "Query completed:  54\n",
      "Query completed:  87\n",
      "Query completed:  94\n",
      "Query completed:  10\n",
      "Query completed:  89\n",
      "Query completed:  61\n",
      "Query completed:  95\n",
      "Query completed:  68\n",
      "Query completed:  57\n",
      "Query completed:  97\n",
      "Query completed:  98\n",
      "Query completed:  60\n",
      "Query completed:  80\n",
      "Query completed:  63\n",
      "Query completed:  91\n"
     ]
    }
   ],
   "source": [
    "## Call each model\n",
    "\n",
    "okapi_result = call_each_model(okapi_model)\n",
    "tfidf_result = call_each_model(tfidf_model)\n",
    "okapi_bm25_result = call_each_model(okapi_bm25)\n",
    "laplace_result = call_each_model(laplace_model)\n",
    "jm_result = call_each_model(jm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to save the result in a file\n",
    "\n",
    "# <query-number> Q0 <docno> <rank> <score> Exp\n",
    "\n",
    "def save_result (store_result_for_each_query, file_name):\n",
    "    with open(file_name,\"w\") as file:\n",
    "        for query_num in store_result_for_each_query:\n",
    "            rank = 1\n",
    "            for docid, score in store_result_for_each_query[query_num]:\n",
    "                temp_str = query_num + \" Q0 \" + str(doc_id_list_mapping[docid]) + \" \" + str(rank) + \" \" +  str(score) + \" \" + \"Exp\"\n",
    "                file.write(temp_str)\n",
    "                file.write(\"\\n\")\n",
    "                rank += 1\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Call to save each model's result\n",
    "\n",
    "save_result (okapi_result, \"okapi\")\n",
    "save_result (tfidf_result, \"okapi_tfidf\")\n",
    "save_result (okapi_bm25_result, \"okapi_bm25\")\n",
    "save_result (laplace_result, \"laplace\")\n",
    "save_result (jm_result, \"jm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
