{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own index to take the place of elasticsearch in the HW1 code, and index the document collection used for HW1. Your index should be able to handle large numbers of documents and terms without using excessive memory or disk I/O.\n",
    "\n",
    "This involves writing two programs:\n",
    "\n",
    "A tokenizer and indexer\n",
    "An updated version of your HW1 ranker which uses your inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from nltk.stem import PorterStemmer\n",
    "from elasticsearch import Elasticsearch \n",
    "global stop_words_removed_stemmed, flatted_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Initialize stemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "es = Elasticsearch()\n",
    "\n",
    "tokenize = nltk.word_tokenize\n",
    "\n",
    "with open (r\"C:\\Users\\mm199\\IR-hw\\HW2_data\\AP_DATA\\stoplist.txt\",\"r\") as file:\n",
    "    file  = file.readlines()\n",
    "stop_words = [i[:-1] for i in file]\n",
    "stop_words_from_nltk = nltk.corpus.stopwords.words(\"english\")\n",
    "stop_words_list = list(set(stop_words_from_nltk + stop_words))\n",
    "\n",
    "pattern =  \"\\w+(\\.?\\w+)*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\mm199\\IR-hw\\HW2_data\\temp_data\\dictfile\",\"rb\") as file:\n",
    "    doc_id_list_mapping_all = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read the data from all sources\n",
    "def get_data():\n",
    "## Get all the docs from the file location\n",
    "    file_all = os.getcwd() + \"\\AP_DATA\\\\ap89_collection\"\n",
    "\n",
    "    ## Remove the read me part\n",
    "    list_of_all = os.listdir(file_all)[:-1]\n",
    "\n",
    "    total_doc = 0\n",
    "    doc_id_list_mapping = {}\n",
    "    for doc in list_of_all:\n",
    "        with open(r\".\\AP_DATA\\ap89_collection\\\\\" + doc, \"r\") as file_data:\n",
    "            file_data_read = file_data.read().split(\"<DOC>\")\n",
    "        for each_file in file_data_read:\n",
    "            if \"</DOCNO>\" in each_file:\n",
    "                doc_id_content = each_file.split(\"</DOCNO>\")\n",
    "                total_doc += 1\n",
    "                doc_id = doc_id_content[0].strip(\"\\n<DOCNO>\").strip()\n",
    "                doc_content = doc_id_content[1].split(\"<TEXT>\")[1].strip(\"\\n</TEXT>\\n</DOC>\\n\").strip()\n",
    "                doc_id_list_mapping[total_doc]= [doc_id, doc_content]\n",
    "    return doc_id_list_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Read queries\n",
    "\n",
    "with open(r\"C:\\Users\\mm199\\IR-hw\\HW1_data\\AP_DATA\\query_desc.51-100.short.txt\", \"r\") as query:\n",
    "    query_list = query.readlines()\n",
    "    \n",
    "## define the common words from queries\n",
    "common_words = ['document', 'discuss', 'exist','determine', 'current','pay','even','taken','type', 'report', 'describe', 'cite', 'include', 'identify', 'make', 'one', 'must', 'second', 'use', 'side', 'take', 'predict']\n",
    "\n",
    "## get all the words in a query\n",
    "all_query_in_a_list = {i[:5].split(\".\")[0]:[match.group().lower() for match in re.finditer(pattern,i[3:],re.M|re.I)] for i in query_list}\n",
    "\n",
    "## words of dictionary which are not in stop words or common words\n",
    "words_list = {}\n",
    "for key in all_query_in_a_list:\n",
    "    if all_query_in_a_list[key] != []:\n",
    "        words_list[key] = [ps.stem(i) for i in all_query_in_a_list[key] if i not in stop_words_list and i not in common_words]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## call the function to get the data\n",
    "doc_id_list_mapping = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  dump the file using pickle\n",
    "def dump(filename, data):\n",
    "    with open(filename,\"wb\") as file:\n",
    "        pickle.dump(data, file)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  load the file using pickle\n",
    "def load(filename):\n",
    "    with open(filename,\"rb\") as file:\n",
    "        data = pickle.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing completed\n",
      "doc_id_list_mapping reduction completed\n",
      "lower function converstion completed\n"
     ]
    }
   ],
   "source": [
    "## change tokenizer refer assignment and remove extra stop words\n",
    "doc_id_list_mapping_all = {i: [match.group() for match in re.finditer(pattern,doc_id_list_mapping[i][1],re.M|re.I)] for i in doc_id_list_mapping}\n",
    "print (\"tokenizing completed\")\n",
    "#doc_id_list_mapping = {i: doc_id_list_mapping[i][0] for i in doc_id_list_mapping}\n",
    "print (\"doc_id_list_mapping reduction completed\")\n",
    "doc_id_list_mapping_all = {i: list(map(str.lower,doc_id_list_mapping_all[i])) for i in doc_id_list_mapping_all}\n",
    "print (\"lower function converstion completed\")\n",
    "# dump(\"doc_id_list_mapping_all\",doc_id_list_mapping_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words_removed_stemmed completed\n",
      "stemmming completed\n"
     ]
    }
   ],
   "source": [
    "stop_words_removed_stemmed = {i : [ps.stem(j) for j in doc_id_list_mapping_all[i] if j not in stop_words_list and j not in string.punctuation] for i in doc_id_list_mapping_all}\n",
    "print (\"stop_words_removed_stemmed completed\")\n",
    "# del(doc_id_list_mapping_all)\n",
    "print (\"stemmming completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words_removed_stemmed1 = {i : [j for j in stop_words_removed_stemmed[i] if not re.fullmatch(\"[\\w]{,2}\",j)] for i in stop_words_removed_stemmed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define and initialize pandas dataframe to save term freq, doc freq and ttf\n",
    "total_doc = len(stop_words_removed_stemmed)\n",
    "list_of_values = list(words_list.values())\n",
    "list_single_item = list(set([i for item in list_of_values for i in item]))\n",
    "\n",
    "df_term_freq = pd.DataFrame(data = 0, index = range(1,total_doc+1), columns = list_single_item)\n",
    "df_ttf_freq = pd.DataFrame(data = 0, index = [\"ttf\"], columns = list_single_item)\n",
    "df_doc_freq = pd.DataFrame(data = 0, index = [\"doc_freq\"], columns = list_single_item)\n",
    "len_of_doc = pd.DataFrame(data = 0, index = range(1,total_doc+1), columns = [\"length\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154634"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Flatten the list\n",
    "flat_list = [j for i in stop_words_removed_stemmed for j in stop_words_removed_stemmed[i]]\n",
    "flatted = list(set(flat_list))\n",
    "flatted_dict = {word : index+1 for index,word in enumerate(flatted)}\n",
    "len(flatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Before saving the data in the file, the term inverted doc must be created\n",
    "\n",
    "def term_wise_dictionary (start, end):  \n",
    "    global stop_words_removed_stemmed, flatted_dict\n",
    "    stop_words_removed_stemmed_batchwise = list(stop_words_removed_stemmed.keys())[start:end]\n",
    "    stitch_dict = {flatted_dict[i]:[] for i in flatted_dict}\n",
    "    for doc_id in stop_words_removed_stemmed_batchwise:\n",
    "        pos_dict = {}\n",
    "        len_of_doc[\"length\"][doc_id] = len(stop_words_removed_stemmed[doc_id])\n",
    "        for index,word in enumerate(stop_words_removed_stemmed[doc_id]):\n",
    "            term_id = flatted_dict[word]\n",
    "            if term_id not in pos_dict.keys():\n",
    "                pos_dict[term_id] = []\n",
    "            pos_dict[term_id].append(index + 1)\n",
    "        for term_id in pos_dict:\n",
    "            doclist = [doc_id, len(pos_dict[term_id])]\n",
    "            doclist.extend(pos_dict[term_id])\n",
    "            stitch_dict[term_id].append(doclist)\n",
    "    stitch_dict = {i:stitch_dict[i] for i in stitch_dict if stitch_dict[i] != []}\n",
    "    return stitch_dict\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step Two: Indexing\n",
    "The next step is to record each document’s tokens in an inverted index. The inverted list for a term must contain the following information:\n",
    "\n",
    "The DF and CF (aka TTF) of the term.\n",
    "A list of IDs of the documents which contain the term, along with the TF of the term within that document and a list of positions within the document where the term occurs. (The first term in a document has position 1, the second term has position 2, etc.)\n",
    "You should also store the following information.\n",
    "\n",
    "The total number of distinct terms (the vocabulary size) and the total number of tokens (total CF) in the document collection.\n",
    "The map between terms and their IDs, if required by your design.\n",
    "The map between document names and their IDs, if required by your design.\n",
    "All inverted lists/files written on the hard drive have to be sorted on DocBlocks by the TF count. This will facilitate merging, in particular with mergesort. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Write the data in a file \n",
    "\n",
    "def write_into_file(filename, term_dict, seek_num = False):\n",
    "    with open(filename, \"a+\") as file:\n",
    "        catalog = {}\n",
    "        for term_id in term_dict:\n",
    "            if seek_num:\n",
    "                file.seek(seek_num)\n",
    "                start_offset = seek_num + 1\n",
    "            else:\n",
    "                start_offset = file.tell()\n",
    "            save_in_the_file = str(term_id) + \":\"\n",
    "            for i in term_dict[term_id]:\n",
    "                temp = str(i).split(\" \")\n",
    "                save_in_the_file += str(\"\".join(temp)).strip(\"\\[\").strip(\"\\]\")  \n",
    "                save_in_the_file += \";\"\n",
    "            file.write(save_in_the_file)\n",
    "            catalog[term_id] = [start_offset, len(save_in_the_file)]\n",
    "    return catalog\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed iteration 10 3.587498903274536\n",
      "Completed iteration 20 3.5397298336029053\n",
      "Completed iteration 30 5.2720301151275635\n",
      "Completed iteration 40 3.805634021759033\n",
      "Completed iteration 50 3.7087371349334717\n",
      "Completed iteration 60 3.6201822757720947\n",
      "Completed iteration 70 3.16208553314209\n",
      "Completed iteration 80 3.8442792892456055\n"
     ]
    }
   ],
   "source": [
    "## Write on file batch wise\n",
    "\n",
    "i = 0\n",
    "j = 0\n",
    "t1 = time.time()\n",
    "while (i < total_doc):\n",
    "    t1 = time.time()\n",
    "    term_dict = term_wise_dictionary(i, i + 1001)\n",
    "    filename_term = \".\\data5\\inverted_dict\" + str(j+1) + \".txt\"\n",
    "    catalog = write_into_file(filename_term, term_dict)\n",
    "    filename =  \".\\data5\\catalog\" + str(j+1)\n",
    "    with open(filename,\"wb\") as file:\n",
    "        pickle.dump(catalog, file)\n",
    "    del(term_dict,catalog)\n",
    "    i += 1000\n",
    "    j += 1\n",
    "    if j % 10 == 0:\n",
    "        print(\"Completed iteration\", j, (time.time() - t1))\n",
    "        t1 = time.time()\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def open_file_and_read_data(term_id, filename, catalog):\n",
    "    start_offset_position = 0\n",
    "    len_position_after_offset = 1\n",
    "    value = catalog[term_id][start_offset_position]\n",
    "    with open(filename, \"r\") as file:\n",
    "        file.seek(value)\n",
    "        row_reader = file.read(catalog[term_id][len_position_after_offset])\n",
    "    read = parser_returns_a_list(row_reader)\n",
    "    return read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parser_returns_a_list(read):\n",
    "    new_dict = {}\n",
    "    for index, i in enumerate(read):\n",
    "        if i == \":\":\n",
    "            semi_colon = index\n",
    "            new_dict[int(read[:semi_colon])] = []\n",
    "            list_to_match = re.split(\";\", read[semi_colon+1:-1])\n",
    "            for i in list_to_match:\n",
    "                if i:\n",
    "                    each_list = re.split(\",\", i)\n",
    "                    each_list = [int(i) for i in each_list]\n",
    "                    new_dict[int(read[:semi_colon])].append(each_list)\n",
    "    return new_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merger_func(filename_catalog1, filename1, filename_catalog2, filename2):\n",
    "    t1 = time.time()\n",
    "    catalog1 = filename_catalog1  \n",
    "    with open(filename_catalog2,\"rb\") as file:\n",
    "        catalog2 = pickle.load(file)\n",
    "    recently_seek_added_address = False\n",
    "    new_filename = \".\\data5\\\\\"+ str(int(t1)) + \".txt\"\n",
    "    new_catalog = {}\n",
    "    term_dict_temp = {}\n",
    "    for term_id in catalog1:\n",
    "        read1 = open_file_and_read_data(term_id, filename1, catalog1)\n",
    "        if term_id in catalog2:\n",
    "            read2 = open_file_and_read_data(term_id, filename2, catalog2)\n",
    "            read_merger = read1[term_id] + read2[term_id]\n",
    "            term_dict_temp[term_id] = read_merger\n",
    "            catalog2.pop(term_id)\n",
    "        else:\n",
    "            term_dict_temp[term_id] = read1[term_id]\n",
    "    del (catalog1)   \n",
    "\n",
    "    for term_id in catalog2:\n",
    "        read2 = open_file_and_read_data(term_id, filename2, catalog2)\n",
    "        term_dict_temp[term_id] = read2[term_id]\n",
    "    del (catalog2)\n",
    "\n",
    "    new_catalog = write_into_file(new_filename, term_dict_temp, recently_seek_added_address)\n",
    "\n",
    "    new_catalog_filename =  \".\\data5\\\\\"+ str(int(t1)) + \"-catalog\"\n",
    "    return new_catalog, new_filename\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time taken: 10.898981809616089\n",
      "Iteration completed:  5 30.60238289833069\n",
      "Iteration completed:  10 180.18739914894104\n",
      "Iteration completed:  15 258.83012223243713\n",
      "Iteration completed:  20 326.6536662578583\n",
      "Iteration completed:  25 396.365051984787\n",
      "Iteration completed:  30 469.6460771560669\n",
      "Iteration completed:  35 538.0184469223022\n",
      "Iteration completed:  40 592.7731511592865\n",
      "Iteration completed:  45 629.7057461738586\n",
      "Iteration completed:  50 700.815577507019\n",
      "Iteration completed:  55 776.6601355075836\n",
      "Iteration completed:  60 852.3921411037445\n",
      "Iteration completed:  65 934.3486547470093\n",
      "Iteration completed:  70 1013.6319983005524\n",
      "Iteration completed:  75 1040.3381969928741\n",
      "Iteration completed:  80 1075.0428161621094\n",
      "Iteration completed:  85 680.6833901405334\n"
     ]
    }
   ],
   "source": [
    "## Call merger on first two batches then repeatedly on all\n",
    "\n",
    "num_of_files = 85\n",
    "\n",
    "t1 = time.time()\n",
    "filename_catalog1 = \".\\data5\\catalog1\" \n",
    "filename_catalog2 = \".\\data5\\catalog2\" \n",
    "\n",
    "filename1 = \".\\data5\\inverted_dict1\" + \".txt\"\n",
    "filename2 = \".\\data5\\inverted_dict2\" + \".txt\"\n",
    "\n",
    "with open(filename_catalog1,\"rb\") as file:\n",
    "    catalog1 = pickle.load(file)   \n",
    "\n",
    "new_catalog, new_filename = merger_func(catalog1, filename1, filename_catalog2, filename2)\n",
    "print (\"time taken:\", time.time() - t1)\n",
    "\n",
    "t1 = time.time()\n",
    "i = 3\n",
    "while (i <= num_of_files):\n",
    "    filename_catalog = \".\\data5\\catalog\"+ str(i)\n",
    "    filename = \".\\data5\\inverted_dict\" + str(i) + \".txt\"\n",
    "    prev_file = \"%s\" %new_filename\n",
    "    prev_catalog = new_catalog\n",
    "    new_catalog, new_filename = merger_func(prev_catalog, prev_file, filename_catalog, filename)\n",
    "    os.remove(prev_file)\n",
    "    i += 1\n",
    "    if i % 5 == 0:\n",
    "        t2 = time.time()\n",
    "        print (\"Iteration completed: \",i, (t2 - t1))\n",
    "        t1 = t2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word completed: 85  time taken :  18.05122685432434  for ids  11016\n",
      "Word completed: 59  time taken :  19.805733680725098  for ids  12179\n",
      "Word completed: 56  time taken :  22.053888082504272  for ids  9772\n",
      "Word completed: 71  time taken :  39.395343542099  for ids  16549\n",
      "Word completed: 64  time taken :  15.598857164382935  for ids  10034\n",
      "Word completed: 62  time taken :  13.129813194274902  for ids  8180\n",
      "Word completed: 93  time taken :  13.471335649490356  for ids  8636\n",
      "Word completed: 99  time taken :  10.044128894805908  for ids  6261\n",
      "Word completed: 58  time taken :  11.896251678466797  for ids  4751\n",
      "Word completed: 77  time taken :  3.5612759590148926  for ids  2088\n",
      "Word completed: 54  time taken :  26.902849912643433  for ids  15022\n",
      "Word completed: 87  time taken :  23.602196216583252  for ids  12137\n",
      "Word completed: 94  time taken :  10.822185754776001  for ids  5997\n",
      "Word completed: 100  time taken :  34.541754961013794  for ids  18039\n",
      "Word completed: 89  time taken :  17.94592595100403  for ids  11183\n",
      "Word completed: 61  time taken :  13.219903469085693  for ids  7714\n",
      "Word completed: 95  time taken :  10.273186445236206  for ids  6838\n",
      "Word completed: 68  time taken :  29.352479696273804  for ids  16774\n",
      "Word completed: 57  time taken :  4.433092832565308  for ids  3107\n",
      "Word completed: 97  time taken :  8.013051748275757  for ids  5233\n",
      "Word completed: 98  time taken :  12.044971942901611  for ids  7755\n",
      "Word completed: 60  time taken :  31.066068410873413  for ids  18359\n",
      "Word completed: 80  time taken :  9.978383302688599  for ids  5896\n",
      "Word completed: 63  time taken :  6.46074652671814  for ids  4697\n",
      "Word completed: 91  time taken :  16.273672103881836  for ids  11173\n"
     ]
    }
   ],
   "source": [
    "## Search es for retrieving doc for each term in a query\n",
    "\n",
    "dict_ids = {}\n",
    "for key in words_list:\n",
    "    list_id = []\n",
    "    t1 = time.time()\n",
    "    for word in words_list[key]:\n",
    "        try:\n",
    "            term_id = flatted_dict[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        val = open_file_and_read_data(term_id = term_id, filename = new_filename, catalog = new_catalog)\n",
    "        list_of_docs_term_present = list(val.values())\n",
    "        list_of_docs_term_present_3000 = sorted(list_of_docs_term_present[0], key = lambda x:x[1],reverse = True)[:2000] \n",
    "        df_doc_freq[word][\"doc_freq\"] = len(list_of_docs_term_present[0])\n",
    "        list_of_docs_term_present_ids = [i[0] for i in list_of_docs_term_present_3000]\n",
    "        for index,doc_id in enumerate(list_of_docs_term_present_ids):\n",
    "            df_term_freq[word][doc_id] = list_of_docs_term_present_3000[index][1]\n",
    "        df_ttf_freq[word][\"ttf\"] = sum([i[1] for i in list_of_docs_term_present[0]])\n",
    "        list_id.extend(list_of_docs_term_present_ids)\n",
    "    dict_ids[key] = list(set(list_id))\n",
    "    print (\"Word completed:\", key,\" time taken : \", (time.time() - t1), \" for ids \", len(dict_ids[key]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Initialize other variables\n",
    "\n",
    "average_doc_length = sum([len(stop_words_removed_stemmed[i]) for i in stop_words_removed_stemmed])/len(stop_words_removed_stemmed)\n",
    "vocab_size = len(flatted_dict)\n",
    "\n",
    "global average_doc_length\n",
    "global total_doc\n",
    "global vocab_size\n",
    "global dict_ids\n",
    "global df_term_freq\n",
    "global df_ttf_freq\n",
    "global df_doc_freq\n",
    "global len_of_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define all model \n",
    "\n",
    "def okapi_model (word_term_freq_list, doc_id):\n",
    "    okapi = 0.0\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        okapi += round((tfwd / (tfwd + 0.5 + 1.5 * (len_of_doc[\"length\"][doc_id] / average_doc_length))),4)\n",
    "    return okapi\n",
    "    \n",
    "def tfidf_model(word_term_freq_list, doc_id):  \n",
    "    tfidf = 0.0\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        doc_freq = df_doc_freq[key][\"doc_freq\"]\n",
    "        if doc_freq == 0:\n",
    "            tfidf_val = 0\n",
    "        else:\n",
    "            tfidf_val = math.log(total_doc / doc_freq)\n",
    "        tfidf += round(((tfwd / (tfwd + 0.5 + 1.5 * (len_of_doc[\"length\"][doc_id] / average_doc_length))) * tfidf_val),4)\n",
    "    return tfidf\n",
    "\n",
    "def okapi_bm25(word_term_freq_list, doc_id):  \n",
    "    bm25 = 0.0\n",
    "    tfwq = {}\n",
    "    k1 = 1.2\n",
    "    k2 = 500\n",
    "    b = 0.75\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        tfwq[key] = word_term_freq_list.count(key)\n",
    "        bm25 += round(((math.log((total_doc + 0.5) / (df_doc_freq[key][\"doc_freq\"] + 0.5)))*((tfwd + k1 * tfwd) / (tfwd + k1 * ((1 - b) + b * (len_of_doc[\"length\"][doc_id] / average_doc_length))))*((tfwq[key] + k2 * tfwq[key]) / (tfwq[key] + k2))), 5)\n",
    "    return bm25 \n",
    "\n",
    "def laplace_model(word_term_freq_list, doc_id):  \n",
    "    laplace = 0.0\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        prob = ((tfwd + 1) / (len_of_doc[\"length\"][doc_id] + (vocab_size)))\n",
    "        laplace += math.log(prob)\n",
    "    return laplace\n",
    "\n",
    "def jm_model(word_term_freq_list, doc_id):\n",
    "    jm = 0.0\n",
    "    epsilon = 0.00000001\n",
    "    lamda = 0.6\n",
    "    for key in word_term_freq_list:\n",
    "        tfwd = df_term_freq[key][doc_id]\n",
    "        ttf = df_ttf_freq[key][\"ttf\"]\n",
    "        if tfwd == 0:\n",
    "            tfwd = epsilon\n",
    "            ttf = epsilon\n",
    "        prob_lm = lamda * (tfwd / len_of_doc[\"length\"][doc_id]) + (1 - lamda) * (ttf / vocab_size)\n",
    "        jm += math.log(prob_lm)\n",
    "    return jm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function for each model per query\n",
    "\n",
    "def call_each_model(model_name):\n",
    "    store_result_for_each_query = {}\n",
    "    for key in words_list:\n",
    "        result_dict = {}\n",
    "        for doc_id in dict_ids[key]:\n",
    "            result_dict[doc_id] = model_name(words_list[key], doc_id)\n",
    "        store_result_for_each_query[key] = sorted(result_dict.items(), key = lambda x:x[1], reverse = True)[:1000]\n",
    "    return store_result_for_each_query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Call each model\n",
    "\n",
    "okapi_result = call_each_model(okapi_model)\n",
    "tfidf_result = call_each_model(tfidf_model)\n",
    "okapi_bm25_result = call_each_model(okapi_bm25)\n",
    "laplace_result = call_each_model(laplace_model)\n",
    "jm_result = call_each_model(jm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Function to save the result in a file\n",
    "\n",
    "# <query-number> Q0 <docno> <rank> <score> Exp\n",
    "\n",
    "def save_result (store_result_for_each_query, file_name):\n",
    "    with open(file_name,\"w\") as file:\n",
    "        global doc_id_list_mapping\n",
    "        for query_num in store_result_for_each_query:\n",
    "            rank = 1\n",
    "            for docid, score in store_result_for_each_query[query_num]:\n",
    "                temp_str = query_num + \" Q0 \" + str(doc_id_list_mapping[docid][0]) + \" \" + str(rank) + \" \" +  str(score) + \" \" + \"Exp\"\n",
    "                file.write(temp_str)\n",
    "                file.write(\"\\n\")\n",
    "                rank += 1\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Call to save each model's result\n",
    "\n",
    "save_result (okapi_result, \".\\data5\\okapi\")\n",
    "save_result (tfidf_result, \".\\data5\\okapi_tfidf\")\n",
    "save_result (okapi_bm25_result, \".\\data5\\okapi_bm25\")\n",
    "save_result (laplace_result, \".\\data5\\laplace\")\n",
    "save_result (jm_result, \".\\data5\\jm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Get updated query\n",
    "\n",
    "with open(r\".\\data\\updated_query.txt\", \"r\", encoding = \"utf8\") as file:\n",
    "    queries = file.readlines()\n",
    "\n",
    "new_updated_query = {(re.match(\"\\d+\",i[:4]).group()): i[3:] for i in queries}\n",
    "new_updated_query_dict = {i : [(ps.stem(j)).lower() for j in [match.group().lower() for match in re.finditer(pattern,new_updated_query[i],re.M|re.I)]] for i in new_updated_query}\n",
    "proximity_result = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word completed: 85  time taken :  0.17756414413452148  for ids  6705\n",
      "Word completed: 59  time taken :  0.04033541679382324  for ids  7245\n",
      "Word completed: 56  time taken :  0.057399749755859375  for ids  9023\n",
      "Word completed: 71  time taken :  0.08496594429016113  for ids  10102\n",
      "Word completed: 64  time taken :  0.07426977157592773  for ids  7614\n",
      "Word completed: 62  time taken :  0.11919069290161133  for ids  9828\n",
      "Word completed: 93  time taken :  0.19465184211730957  for ids  10494\n",
      "Word completed: 99  time taken :  0.02768564224243164  for ids  7225\n",
      "Word completed: 58  time taken :  0.047683000564575195  for ids  9009\n",
      "Word completed: 77  time taken :  0.0  for ids  2088\n",
      "Word completed: 54  time taken :  0.04686880111694336  for ids  8921\n",
      "Word completed: 87  time taken :  0.11591172218322754  for ids  12803\n",
      "Word completed: 94  time taken :  0.04264497756958008  for ids  6512\n",
      "Word completed: 100  time taken :  0.07690787315368652  for ids  10222\n",
      "Word completed: 89  time taken :  0.16534972190856934  for ids  9891\n",
      "Word completed: 61  time taken :  0.060898780822753906  for ids  5613\n",
      "Word completed: 95  time taken :  0.025964975357055664  for ids  7740\n",
      "Word completed: 68  time taken :  0.031243085861206055  for ids  4137\n",
      "Word completed: 57  time taken :  0.03477668762207031  for ids  5919\n",
      "Word completed: 97  time taken :  0.01562666893005371  for ids  3161\n",
      "Word completed: 98  time taken :  0.04686427116394043  for ids  5315\n",
      "Word completed: 60  time taken :  1.5767967700958252  for ids  10879\n",
      "Word completed: 80  time taken :  0.12059187889099121  for ids  11208\n",
      "Word completed: 63  time taken :  0.07198929786682129  for ids  8136\n",
      "Word completed: 91  time taken :  0.09406757354736328  for ids  13643\n"
     ]
    }
   ],
   "source": [
    "## Search es for retrieving doc for each term in a query\n",
    "\n",
    "dict_ids_prox = {}\n",
    "for key in new_updated_query_dict:\n",
    "    list_id = []\n",
    "    t1 = time.time()\n",
    "    for word in new_updated_query_dict[key]:\n",
    "        try:\n",
    "            term_id = flatted_dict[word]\n",
    "        except KeyError:\n",
    "            continue\n",
    "        val = open_file_and_read_data(term_id = term_id, filename = new_filename, catalog = new_catalog)\n",
    "        list_of_docs_term_present = list(val.values())\n",
    "        list_of_docs_term_present_3000 = sorted(list_of_docs_term_present[0], key = lambda x:x[1],reverse = True)[:5000]\n",
    "        list_of_docs_term_present_ids = [i[0] for i in list_of_docs_term_present_3000]\n",
    "        list_id.extend(list_of_docs_term_present_ids)\n",
    "    dict_ids_prox[key] = list(set(list_id))\n",
    "    print (\"Word completed:\", key,\" time taken : \", (time.time() - t1), \" for ids \", len(dict_ids_prox[key]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# (C - rangeOfWindow) * numOfContainTerms / (lengthOfDocument + V)\n",
    "\n",
    "# where C is a parameter which can be set to 1500 (you can play with C to achieve better results).\n",
    "\n",
    "# rangeOfWindow refers to the output from the algorithm - http://stevekrenzel.com/articles/blurbs\n",
    "\n",
    "# numOfContainTerms refers to the number of query terms contained by the document.\n",
    "\n",
    "# lengthOfDocument - document length\n",
    "\n",
    "# V - vocabulary size\n",
    "\n",
    "def proximity_search_model(list_of_positions, doc_id, num_query_terms_contained_by_doc):\n",
    "    C = 1500\n",
    "    range_of_window = prox_search(list_of_positions)   \n",
    "    prox_result =  ((C - range_of_window) * num_query_terms_contained_by_doc)/(len_of_doc[\"length\"][doc_id] + vocab_size)\n",
    "    if doc_id % 10000 == 0:\n",
    "        print (list_of_positions,range_of_window, doc_id, num_query_terms_contained_by_doc, len_of_doc[\"length\"][doc_id], prox_result)\n",
    "    return prox_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Blurb algorithm\n",
    "\n",
    "def prox_search(list_of_positions):\n",
    "    len_l = len(list_of_positions)\n",
    "    temp_dict = {i:False for i in range(len(list_of_positions))}\n",
    "    total_trials = sum([len(j) for j in list_of_positions])\n",
    "    range_val = float(\"inf\")\n",
    "    ## Initialize value_list with only the first element of each position list\n",
    "    val_list = [list_of_positions[key][0] for key in range (len(list_of_positions))]\n",
    "    if len_l == 1:\n",
    "        return 0\n",
    "    while (True):\n",
    "        temp_val_list = [i for i in temp_dict if temp_dict[i] == False]\n",
    "        if len(temp_val_list) == 1:\n",
    "            break   \n",
    "        increment_min = float(\"inf\")\n",
    "        for j in temp_val_list:\n",
    "            if val_list[j] < increment_min:\n",
    "                min_pointer = j\n",
    "                increment_min =  val_list[j]\n",
    "        current_max = max(val_list)\n",
    "        current_min = min(val_list)\n",
    "        if range_val > (current_max - current_min) and current_max !=  float(\"-inf\"):\n",
    "            range_val = current_max - current_min   \n",
    "\n",
    "        current_i = list_of_positions[min_pointer].index(increment_min)\n",
    "        try:\n",
    "            increment_val = list_of_positions[min_pointer][current_i + 1]\n",
    "            val_list[min_pointer] = increment_val\n",
    "        except IndexError:\n",
    "            temp_dict[min_pointer] = True\n",
    "            \n",
    "    return range_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18, 159, 198], [1160], [2320]] 2122 10000 1 290 -0.004014871808112365\n",
      "[[22, 87, 94, 102], [720], [1440]] 1338 30000 1 180 0.0010464169907114344\n",
      "[[14, 31, 41, 202, 257], [1068], [2136]] 1879 40000 1 267 -0.002446724036642759\n",
      "[[29], [1160], [2320]] 2291 10000 1 290 -0.005105729260798843\n",
      "[[6], [864], [1728]] 1722 60000 1 216 -0.0014336454633516306\n",
      "[[242, 254], [1112], [2224]] 1970 50000 1 278 -0.003033980582524272\n",
      "[[105], [736], [1472]] 1367 70000 1 184 0.0008590732343784314\n",
      "[[13, 79, 92, 104, 147, 174, 186, 217, 227, 238], [1160], [2320]] 2082 10000 1 290 -0.0037566806950504765\n",
      "[[105], [736], [1472]] 1367 70000 1 184 0.0008590732343784314\n",
      "[[99], [736], [1472]] 1373 70000 1 184 0.0008203180508726376\n",
      "[[7, 48], [720], [1440]] 1392 30000 1 180 0.0006976113271409563\n",
      "[[199], [1068], [2136]] 1937 40000 1 267 -0.002821156738820279\n",
      "[[221], [1068], [2136]] 1915 40000 1 267 -0.0026791305414425987\n",
      "[[34], [1112], [2224]] 2190 50000 1 278 -0.004454141706259037\n",
      "[[74], [736], [1472]] 1398 70000 1 184 0.0006588381195984963\n"
     ]
    }
   ],
   "source": [
    "store_result_for_each_query = {}\n",
    "dict_of_positions_present = {}\n",
    "for key in new_updated_query_dict:\n",
    "    dict_of_positions_present[key] = []\n",
    "    for word in new_updated_query_dict[key]:\n",
    "        term_id = flatted_dict[word]\n",
    "        val = open_file_and_read_data(term_id = term_id, filename = new_filename, catalog = new_catalog)\n",
    "        dict_ = {i[0]:i[2:] for i in list(val.values())[0] if i[0] in dict_ids_prox[key]}\n",
    "        dict_of_positions_present[key].extend(sorted(dict_.items(), key = lambda x:len(x[1]), reverse = True))\n",
    "    #dict_of_positions = [i for j in dict_of_positions_present for i in dict_of_positions_present[j]]\n",
    "    sorted_v = sorted(dict_of_positions_present[key], key = lambda x:x[0])\n",
    "    another_dict = {}\n",
    "    i = 0\n",
    "    while(i < len(sorted_v) - 2):\n",
    "        no_val = [(len_of_doc[\"length\"][sorted_v[i][0]])*4]\n",
    "        if sorted_v[i][0] == sorted_v[i+1][0]:\n",
    "            if sorted_v[i+1][0] == sorted_v[i+2][0]:\n",
    "                another_dict[sorted_v[i][0]] = [sorted_v[i][1], sorted_v[i+1][1], sorted_v[i+2][1]]\n",
    "                i += 3\n",
    "            else:\n",
    "                another_dict[sorted_v[i][0]] = [sorted_v[i][1], sorted_v[i+1][1], no_val]\n",
    "                i += 2\n",
    "        else:\n",
    "            another_dict[sorted_v[i][0]] = [sorted_v[i][1], no_val, [no_val[0] * 2]]\n",
    "            i += 1        \n",
    "    result_dict = {}\n",
    "    for doc_id in another_dict:\n",
    "        no_val = (len_of_doc[\"length\"][doc_id])*4\n",
    "        \n",
    "        num_query_terms_contained_by_doc = [i for i in another_dict[doc_id] if i not in [[no_val],[no_val * 2]]]\n",
    "        result_dict[doc_id] = proximity_search_model(another_dict[doc_id], doc_id, len(num_query_terms_contained_by_doc))\n",
    "    store_result_for_each_query[key] = sorted(result_dict.items(), key = lambda x:x[1], reverse = True)[:1000]\n",
    "    \n",
    "save_result (store_result_for_each_query, \".\\data5\\prox_temp\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ## change not present terms\n",
    "# ## Call proximity search and store the result\n",
    "\n",
    "# store_result_for_each_query = {}\n",
    "# dict_of_positions_present = {}\n",
    "# for key in new_updated_query_dict:\n",
    "#     dict_of_positions_present[key] = []\n",
    "#     for word in new_updated_query_dict[key]:\n",
    "#         try:\n",
    "#             term_id = flatted_dict[word]\n",
    "#         except:\n",
    "#             continue\n",
    "#         val = open_file_and_read_data(term_id = term_id, filename = new_filename, catalog = new_catalog)\n",
    "#         dict_ = {i[0]:i[2:] for i in list(val.values())[0] if i[0] in dict_ids_prox[key]}\n",
    "#         dict_of_positions_present[key].extend(sorted(dict_.items(), key = lambda x:len(x[1]), reverse = True))\n",
    "#     sorted_v = sorted(dict_of_positions_present[key], key = lambda x:x[0])\n",
    "#     another_dict = {i[0]:[] for i in sorted_v}\n",
    "#     for i in sorted_v:\n",
    "#         another_dict[i[0]].append(i[1])        \n",
    "#         result_dict = {}\n",
    "#     for doc_id in another_dict:\n",
    "#         result_dict[doc_id] = proximity_search_model(another_dict[doc_id], doc_id, len(another_dict[doc_id]))\n",
    "#     store_result_for_each_query[key] = sorted(result_dict.items(), key = lambda x:x[1], reverse = True)[:1000]\n",
    "    \n",
    "# save_result (store_result_for_each_query, \".\\data4\\prox4\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook HW2-IR.ipynb to python\n",
      "[NbConvertApp] Writing 23480 bytes to HW2-IR.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python HW2-IR.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
